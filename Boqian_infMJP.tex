%%
%% This is file `docultex.tex', 
%% Documentation for siam supplemental file macros for use with LaTeX 2e
%% 
%% December 19, 2013
%%
%% Supplemental Files Supported Version 1.0.0
%% 
%% You are not allowed to change this file. 
%% 
%% You are allowed to distribute this file under the condition that 
%% it is distributed together with all of the files in the siam macro 
%% distribution. These are:
%%
%%  siamltex1213.cls (main LaTeX macro for SIAM)
%%  siam10.clo   (size option for 10pt papers)
%%  subeqn.clo   (allows equation numbners with lettered subelements)
%%  siam.bst     (bibliographic style file for BibTeX)
%%  docultex.tex (this file)
%%
%% If you receive only some of these files from someone, complain! 
%% 
%% You are NOT ALLOWED to distribute this file alone. You are NOT 
%% ALLOWED to take money for the distribution or use of either this 
%% file or a changed version, except for a nominal charge for copying 
%% etc. 
%% \CharacterTable
%%  {Upper-case    \A\B\C\D\E\F\G\H\I\J\K\L\M\N\O\P\Q\R\S\T\U\V\W\X\Y\Z
%%   Lower-case    \a\b\c\d\e\f\g\h\i\j\k\l\m\n\o\p\q\r\s\t\u\v\w\x\y\z
%%   Digits        \0\1\2\3\4\5\6\7\8\9
%%   Exclamation   \!     Double quote  \"     Hash (number) \#
%%   Dollar        \$     Percent       \%     Ampersand     \&
%%   Acute accent  \'     Left paren    \(     Right paren   \)
%%   Asterisk      \*     Plus          \+     Comma         \,
%%   Minus         \-     Point         \.     Solidus       \/
%%   Colon         \:     Semicolon     \;     Less than     \<
%%   Equals        \=     Greater than  \>     Question mark \?
%%   Commercial at \@     Left bracket  \[     Backslash     \\
%%   Right bracket \]     Circumflex    \^     Underscore    \_
%%   Grave accent  \`     Left brace    \{     Vertical bar  \|
%%   Right brace   \}     Tilde         \~}

\documentclass[final,leqno,onefignum,onetabnum]{siamltex1213}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb,mathabx}
\title {Boqian MJPS\thanks{This work was
supported by the Society for Industrial and Applied
Mathematics}} 

\author{Author\thanks{Society for Industrial and
Applied Mathematics, Philadelphia, Pennsylvania. 
(\email{tex@siam.org}). Questions, comments, or corrections
to this document may be directed to that email address.}}

\begin{document}
\maketitle
\slugger{mms}{xxxx}{xx}{x}{x--x}%slugger should be set to mms, siap, sicomp, sicon, sidma, sima, simax, sinum, siopt, sisc, or sirev

\begin{abstract}

\end{abstract}

\begin{keywords}
MCMC, Markov Jump Process,  Slice Sampling, infinite state space
\end{keywords}
%\begin{AMS}\end{AMS}

\pagestyle{myheadings}
\thispagestyle{plain}
\markboth{MJPs}{MJPs}
\section{Beam Sampling for continuous time Infinite Hidden Markov Models}~
Model Assumptions:\\
We are considering a continuous time Infinite Hidden Markov Model (iHMM), with transition matrix $A = (A_{ij})_{i, j \geq 1}$.\\
\begin{align*}
A_i \doteq A_{ii} = -\sum_{j \neq i} A_{ij}\\
A_{ij} > 0 \;, i \neq j
\end{align*}
Now, we are using the way in reference 1 to construct a MJPs with virtual jumps. \\
\begin{algorithm}[tb]
   \caption{State-dependent thinning for MJPs }
   \label{alg:State-dependent thinning for MJPs}
\begin{algorithmic}
   \STATE {\bfseries Input:} Transition matrix $A_{ss'}$, and an initial distribution over states $\pi_0$.\\
   Dominating Transition Rate Vector $B_s \geq A_s.$
   \STATE {\bfseries Output:} A piecewise constant trajectory $(V, W) = ((v_i, w_i)) $ on the time interval $[t_{start}, t_{end}].$\\
   \STATE Initialize, 
   \\ Draw $v_0 \sim \pi_0$ and set $w_0 = t_{start}$. Set $i = 0$. 
   \WHILE{$w_i < t_{end}$ }
	\STATE (a) Sample $\tau_i \sim B_{v_i}$.\\
	\STATE (b) Set $v_{i + 1} = v_i$ with probability $1 - \frac{A_{v_i}}{B_{v_i}}$ and set $w_{i + 1} = w_i + \tau_i$.\\
	\STATE (c) \textbf{Else:} Set $w_{i + 1} = w_i + \tau_i$ and sample $v_{i + 1}$ with $P(v_{i + 1} = s | v_i) = A_{v_i, s} / A_{v_i}$.\\
	\STATE (d) Incresement i.
   \ENDWHILE
\end{algorithmic}
\end{algorithm}

\begin{proposition}
The path $(W, V)$ returned by the thinning procedure described in algorithm 3 is equivalent to a sample $(S, T)$ from the $MJP(\pi_0, A)$.\\
\end{proposition}
\begin{proof}
$S = (s_0, s_1, ... , s_N)\;, T = (t_0, t_1,...,t_N, t_{N +1})$. And let's call the virtual jumps as $U$. Denote the virtual jump times between $(t_i, t_{i + 1})$ as $n_i$. Then the density function of $(W, V)$ will be as follows.
\begin{align*}
P(W, V) &= \pi_0(s_0) \prod_{i = 0}^{N - 1}\exp(-B_{s_i}(t_{i + 1} - t_i))B_{s_i}^{n_i}(1 - \frac{A_{s_i}}{B_{s_i}})^{n_i} B_{s_i}\frac{A_{s_i}}{B_{s_i}} \cdot  \exp(-B_{s_N}(t_{N + 1} - t_N))B_{s_N}^{n_N}(1 - \frac{A_{s_N}}{B_{s_N}})^{n_N} \\
&= \pi_0(s_0)\exp(-\int_{t_0}^{t_{N+1}}B_{S_{(t)}}dt)\prod_{i = 0}^N (B_{s_i} - A_{s_i})^{n_i} \prod_{i = 0}^{N - 1} A_{s_is_{i+1}}\\
\end{align*}
So after integrating with respect to virtual jump times and the numbers of virtual jumps, we can get the following.\\
\begin{align*}
P(S, T) &= \sum_{n_1,n_2,...,n_N \geq 0} \int_{t_0 \leq \tau_1^1\leq...\leq \tau_{n_1}^1\leq t_1}...\int_{t_N \leq \tau_1^N\leq...\leq \tau_{n_N}^N \leq t_{N+1}} P(W,V)d\tau_1^1...d\tau_{n_1}^1...d\tau_1^{N}...d\tau_{n_N}^N \\ &= \pi_0(s_0) \exp(-\int_{t_0}^{t_{N+1}}A_{S_{(t)}}dt) \prod_{i = 0}^{N - 1}A_{s_is_{i + 1}}
\end{align*}
So the proposition is proved.\\
\end{proof}\\
The main idea of beam sampler for infinite-state continuous time Hidden Markov Model is to introduce auxiliary variables $\mathbb{\mu}$ such that conditioned on $\mathbb{\mu}$, the number of trajectories with positive probability is finite. Then dynamic programming can be used to compute the conditional probabilities efficiently. \\
Assume $W = (w_0, w_1,...,w_{N'}, w_{N' + 1})\;, V=(v_0, v_1,...,v_{N'})\;, \mu = (\mu_1, \mu_2,...,\mu_{N'}).$\\
\begin{align*}
P(\mu | W, V) = \prod_{i = 1}^{N'}\frac{\mathbb{I}(0 \leq \mu_i \leq \frac{A_{v_{i - 1}v_{i}}}{A_{v_{i - 1}}})}{\frac{A_{v_{i - 1}v_{i}}}{A_{v_{i - 1}}}}
\end{align*}
It indicates that conditioned on the trajectory$(V, W)$, $\mu_i $ is depending on $A$, $v_i$, and $v_{i - 1}$ and  $\mu_i \sim Uniform(0, \frac{A_{v_{i - 1}v_i}}{A_{v_{i- 1}}})$.\\

\begin{proposition}
Conditioned on a trajectory $(S, T)$ of the MJP, the virtual jump times $U$ are distributed as a Poisson process with density $B_{s(t)} - A_{s(t)}$.\\
\end{proposition}
\begin{proof}
$S = (s_0, s_1, ... , s_N)\;, T = (t_0, t_1,...,t_N, t_{N +1})$. And let's call the virtual jumps as $U$. Denote the virtual jump times between $(t_i, t_{i + 1})$ as $n_i$. Then the density function of $(W, V)$ will be as follows.\\
\begin{align*}
P(W,V) &= P(U, S, T) \\&= \pi_0(s_0)\exp(-\int_{t_0}^{t_{N+1}}B_{S_{(t)}}dt)\prod_{i = 0}^N (B_{s_i} - A_{s_i})^{n_i} \prod_{i = 0}^{N - 1} A_{s_is_{i+1}}\\
\end{align*}
\begin{align*}
P(S, T, n_0, ..., n_N) &= \pi_0(s_0)\exp(-\int_{t_0}^{t_{N+1}}B_{S_{(t)}}dt)\prod_{i = 0}^N\frac{((B_{s_i} - A_{s_i})(t_{i + 1} - t_i))^{n_i}}{n_i !} \prod_{i = 0}^{N - 1} A_{s_is_{i+1}}\\
\end{align*}
So the conditional probability $P(n_0, n_1,...,n_N | S,T )$ will be as follows.\\
\begin{align*}
P(n_0, ..., n_N | S, T) &= \exp(-\int_{t_0}^{t_{N+1}}(B_{S_{(t)}} - A_{S_{(t)}}) dt)\prod_{i = 0}^N\frac{((B_{s_i} - A_{s_i})(t_{i + 1} - t_i))^{n_i}}{n_i !}\\
\end{align*}
So it indicates that conditioned on the trajectory $(S, T)$, the virtual jump $U$ is distributed as a non-homogeneous Poisson process with density $B_{s(t)} - A_{s(t)}$.
\end{proof}\\
\textbf{Sampling v}: Using the same trick used in Beam Sampling for the Infinite HMM, we can sample $P(v_t | y, \mu, W)$. So can we sample $P(v_t| v_{t + 1}, y , W, u)$.\\
\indent First of all, consider $P(v_i | y_{w_0, w_{i + 1}}, w_{0: i}, \mu_{0: i})$.\\
\begin{align*}
P(v_i , w_{0: i}, \mu_{0: i}, y_{[w_0, w_{i + 1})}) &= \sum_{v_{i-1}} P(v_i,y_{[w_i, w_{i + 1})}, w_i, \mu_i,  v_{i - 1}, w_{0:i-1},\mu_{0:i - 1}, y_{[w_0, w_i )})\\
&= \sum_{v_{i-1}} P(v_i,y_{[w_i, w_{i + 1})}, w_i, \mu_i|  v_{i - 1}, w_{0:i-1},\mu_{0:i - 1}, y_{[w_0, w_i )}) P(v_{i - 1}, w_{0:i-1},\mu_{0:i - 1}, y_{[w_0, w_i )})\\
&= \sum_{v_{i-1}} P(y_{[w_i, w_{i + 1})} | v_i, w_i, w_{i+1}) P(\mu_i | v_i, v_{i - 1}) P(v_i, w_i |v_{i - 1},w_{i - 1})P(v_{i - 1}, w_{0:i-1},\mu_{0:i - 1}, y_{[w_0, w_i )})\\
&=P(y_{[w_i, w_{i + 1})} | v_i, w_i, w_{i+1}) \sum_{v_{i-1}} \frac{\mathbb{I}(0 \leq \mu_i \leq \frac{A_{v_{i-1}v_i}}{A_{v_{i-1}}})}{\frac{A_{v_{i-1}v_i}}{A_{v_{i-1}}}} \exp(-B_{v_{i-1}}(w_i - w_{i-1}))\\&(B_{v_{i - 1}} - A_{v_{i - 1}})^{\mathbb{I}(v_i = v_{i-1})}A_{v_{i - 1}v_i}^{\mathbb{I}(v_i  v_{i-1})} P(v_{i - 1}, w_{0:i-1},\mu_{0:i - 1}, y_{[w_0, w_i)})\\
&=P(y_{[w_i, w_{i + 1})} | v_i, w_i, w_{i+1}) \sum_{\Im_{i-1}} \frac{\mathbb{I}(0 \leq \mu_i \leq \frac{A_{v_{i-1}v_i}}{A_{v_{i-1}}})}{\frac{A_{v_{i-1}v_i}}{A_{v_{i-1}}}} \exp(-B_{v_{i-1}}(w_i - w_{i-1}))\\&(B_{v_{i - 1}} - A_{v_{i - 1}})^{\mathbb{I}(v_i = v_{i-1})}A_{v_{i - 1}v_i}^{\mathbb{I}(v_i  v_{i-1})} P(v_{i - 1}, w_{0:i-1},\mu_{0:i - 1}, y_{[w_0, w_i)})
\end{align*}
\indent Although the summation over $v_{i-1}$ is an infinite sum, the auxiliary variable $\mu_i$ truncates this summation to the finitely many $v_{i-1}$'s  and $v_i$'s that satisfy both constrains $\mu_i \leq \frac{A_{v_{i-1}v_i}}{A_{v_{i-1}}}$ and $P(v_{i - 1} | y_{[w_0, w_i)}, \mu_{0: i-1}) > 0$. This means that $|\Im_{i-1}| < +\infty$.\\
\indent Secondly, consider $P(v_i | v_{i + 1}, y_{w_0, w_{N' + 1}}, w_{0: N' + 1}, \mu_{0: N'})$.\\
\begin{align*}
&P(v_i | v_{i + 1}, y_{w_0, w_{N' + 1}}, w_{0: N' + 1}, \mu_{0: N'}) \propto P(v_i , v_{i + 1}, y_{w_0, w_{N' + 1}}, w_{0: N' + 1}, \mu_{0: N'})\\
&= P(y_{[w_{i+1}, w_{N'+1})}, \mu_{i+2: N}, w_{i+2 : N}| v_i, v_{i+1}, y_{[w_0, w_{i + 1}), w_{0: i+1}, \mu_{0:i + 1})}) P(v_i, v_{i+1}, y_{[w_0, w_{i+1})}, w_{0: i+1}, \mu_{0:i + 1})\\
&=P(y_{[w_{i+1}, w_{N'+1})}, \mu_{i+2: N}, w_{i+2 : N}| v_{i+1}, w_{i+1}) P(v_i, v_{i+1}, y_{[w_0, w_{i+1})}, w_{0: i+1}, \mu_{0:i + 1})\\
&= Const \cdot P(v_i, v_{i+1}, y_{[w_0, w_{i+1})}, w_{0: i+1}, \mu_{0:i + 1})\\
&= Const \cdot P(v_{i+1}, u_{i+1}, w_{i+1}| v_i, y_{[w_0, w_{i+1})}, w_{0: i}, \mu_{0:i}) \cdot P( v_i, w_{0: i}, \mu_{0:i}, y_{[w_0, w_{i+1})})\\
&= Const \cdot P(v_{i+1}, u_{i+1}, w_{i+1}| v_i, w_{0: i}, \mu_{0:i}) \cdot P( v_i, w_{0: i}, \mu_{0:i}, y_{[w_0, w_{i+1})})
\end{align*} 
\indent Finally, to sample the complete trajectory, we can sample $P(v_{N'} | y_{w_0, w_{N'+1}}, \mu_{0: N'})$ first, and then do a backward sampling using the above formula.\\ 
\begin{algorithm}[tb]
   \caption{Beam Sampler for continuous time Infinite Hidden Markov Models }
   \label{alg:Beam Sampler for continuous time Infinite Hidden Markov Models}
\begin{algorithmic}
   \STATE {\bfseries Input:} observations $y_{[t_0, t_{k+1})}$, $A$, $B$, $\pi_0$
   \STATE Initialize, $i = 0$
   \\ (a) Set current trajectory $[S,T](0)$ arbitrarily.\\
   \REPEAT
   \FOR{$i=0$ {\bfseries to} $N$}
	\STATE (a) Sample virtual jumps $U(i + 1) \sim Poisson\; Process(B_{s(t)} - A_{s(t)})$, given $S(i), T(i)$.\\
	\STATE (b) Sample $\mu(i + 1)_j \sim Uniform(0, \frac{A_{v_{j - 1}v_j}}{A_{v_{j- 1}}})$, $j = 1,2,...,N'$.\\
	\STATE (c) Sample $V( i+ 1) \sim P(V | W(i + 1), \mu(i + 1), y)$\\
	\STATE (d) Delete all the virtual jumps to get $S(i + 1), \; T(i + 1)$\\
   \ENDFOR
   \UNTIL{$ i = N$ }
\end{algorithmic}
\end{algorithm}
\begin{theorem}
Algorithm 4 has $P(S,T, W, \mu | y)$ as a stationary distribution.\\
\end{theorem}
\begin{proof}
Firstly, prove (c) step has $P(S,T | W, \mu, y)$ as a stationary distribution. It comes from the following detail balance condition.\\
\begin{align*}
P((W,S, T,\mu) \rightarrow (W, S^*, T^*, \mu))P(S, T| W, \mu,y) &= P(V^*| W, \mu, y)P(V | W, \mu, y)\\
&=P((W,S^*, T^*,\mu) \rightarrow (W, S, T, \mu))P(S^*, T^*| W, \mu,y)\\
\end{align*}
Secondly, prove (a) and (b) step have $P(W, \mu | S,T,y )$ as a stationary distribution.\\
\begin{align*}
P(W, \mu| S,T,y) &=P(U, \mu | S, T, y)= \frac{P(U, \mu, S, T, y)}{P(S,T,y)}\\
&= \frac{P(y|S,T)P(U, \mu, S,T)}{P(y|S,T)P(S,T)} = P(\mu| S, T, U)P(U|S,T)\\
&= P(\mu|V,W)P(U|S,T)
\end{align*}
We know the transition probability $P((S,T,W,\mu)\rightarrow (S,T,W^*,\mu^*))$ is as follows.\\
\begin{align*}
P((S,T,W,\mu)\rightarrow (S,T,W^*,\mu^*)) &=  P(\mu^* |V^*, W^* )P(U^* |S,T) \\
&= P(\mu^*| S, T, U^*)P(U^*|S,T) = P(W^*, \mu^*| S,T,y)
\end{align*}
So step(a) and (b) have $P(W, \mu | S,T,y )$ as a stationary distribution.\\
Above all, this theorem is proved.\\
\end{proof}

\section{Figures and tables}
\section{Bibliography and Bib\TeX}
\section{Conclusion} 
\Appendix
\section{The use of appendices}
\appendix
\section{Title of appendix}

\begin{thebibliography}{1}
\bibitem{ViRaYeWhTe} {\sc Vinayak Rao, Yee Whye Teh},
{\em MCMC for continuous-time discrete-state systems}, NIPS, 2012.

\end{thebibliography}


\end{document}
%% end of file `docultex.tex'
